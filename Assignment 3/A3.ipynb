{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "PpcoqA-3l2g9",
      "metadata": {
        "id": "PpcoqA-3l2g9"
      },
      "source": [
        "## Installing Sentence Transsformer and other models/frameworks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0tw0xe31hT59",
      "metadata": {
        "id": "0tw0xe31hT59"
      },
      "outputs": [],
      "source": [
        "# pip install -U sentence-transformers\n",
        "# pip install gensim\n",
        "\n",
        "\n",
        "# Kindly add all your installations and versions if any in this cell."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WWWTQtXDl-t8",
      "metadata": {
        "id": "WWWTQtXDl-t8"
      },
      "source": [
        "## Importing necessary libraries. \n",
        "In the final version all imports should be stricly enlisted here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "e951f9be",
      "metadata": {
        "id": "e951f9be"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import multiprocessing\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "import gensim\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# import spacy\n",
        "# from scipy import stats\n",
        "from sklearn import linear_model\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, losses, models, util\n",
        "# from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "# from sentence_transformers.readers import InputExample\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "# import torch \n",
        "# from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H5Ch9I58mMGe",
      "metadata": {
        "id": "H5Ch9I58mMGe"
      },
      "source": [
        "## Load dataset: 7 marks\n",
        "1 Download and unzip the dataset from this link http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz  **1 mark**\n",
        "\n",
        "2 Complete the code in `read_sts_csv()`. **4.5 marks**\n",
        "\n",
        "3 Create 3 dataframes one each for train, test and val and print their final shapes. **1.5 marks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2TMR0Z0DlfFf",
      "metadata": {
        "id": "2TMR0Z0DlfFf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev: (1500, 7)\n",
            "train: (5748, 7)\n",
            "test: (1379, 7)\n",
            "0                                  A plane is taking off.\n",
            "1                         A man is playing a large flute.\n",
            "2           A man is spreading shreded cheese on a pizza.\n",
            "3                            Three men are playing chess.\n",
            "4                             A man is playing the cello.\n",
            "                              ...                        \n",
            "5744           Severe Gales As Storm Clodagh Hits Britain\n",
            "5745    Dozens of Egyptians hostages taken by Libyan t...\n",
            "5746                         President heading to Bahrain\n",
            "5747           China, India vow to further bilateral ties\n",
            "5748     Putin spokesman: Doping charges appear unfounded\n",
            "Name: sent_a, Length: 5748, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# For preprocessing in sts-train.csv, we removed \"Europe Media Monitor (http://emm.newsbrief.eu)\" from the entire csv file\n",
        "# It reduced the number of buggy lines to less than 10 from more than 200.\n",
        "\n",
        "# For preprocessing in sts-dev.csv, we removed \"Europe Media Monitor (http://emm.newsbrief.eu)\" from the entire csv file\n",
        "# It reduced the number of buggy lines to less than 10 from more than 40.\n",
        "\n",
        "# For preprocessing in sts-test.csv, we removed \"Europe Media Monitor (http://emm.newsbrief.eu)\" from the entire csv file along with all mentions of stack exchange links.\n",
        "# It reduced the number of buggy lines from more than 300 to close to 0.\n",
        "\n",
        "def read_sts_csv(dataset_type=\"dev\", columns=['source', 'type', 'year', 'id', 'score', 'sent_a', 'sent_b']):\n",
        "  path = INPUT_PATH + \"/sts-\"+ dataset_type + \".csv\"\n",
        "  df = pd.read_csv(path, sep='\\t', header=None, on_bad_lines = 'skip',lineterminator='\\n',quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
        "  df.columns = columns\n",
        "  # df.describe()\n",
        "  return df\n",
        "  \n",
        "INPUT_PATH = r\"stsbenchmark\"\n",
        "df_dev = read_sts_csv(\"dev\") # create the train, dev and test dataframes\n",
        "df_train = read_sts_csv(\"train\")\n",
        "df_test = read_sts_csv(\"test\")\n",
        "df_train = df_train.dropna()\n",
        "print(\"dev:\",df_dev.shape)\n",
        "print(\"train:\",df_train.shape)\n",
        "print(\"test:\",df_test.shape)\n",
        "print(df_train['sent_a'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gO2ZkIwDmo4s",
      "metadata": {
        "id": "gO2ZkIwDmo4s"
      },
      "source": [
        "## Hyperparameters: 5 Marks\n",
        "Update this cell with you choosen parameters except, NUM_EPOCHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5b3dddd6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n"
          ]
        }
      ],
      "source": [
        "cores = multiprocessing.cpu_count() \n",
        "print(cores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4QurhOG7E0Z-",
      "metadata": {
        "id": "4QurhOG7E0Z-"
      },
      "outputs": [],
      "source": [
        "\n",
        "HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL ='sentence-transformers/all-mpnet-base-v2' # USE THE HUGGAING FACE VERSION OF SENTENCE_TRANSFORMER_TYPE\n",
        "NON_CONEXTUAL_MODEL_TYPE = 'Word2Vec'\n",
        "CONEXTUAL_MODEL_TYPE = HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL\n",
        "INPUT_PATH = r\"stsbenchmark\"\n",
        "BATCH_SIZE = 1024\n",
        "OUT_DIM_DENSE = 128\n",
        "NUM_EPOCHS = 2 ## THIS IS FIXED DO NOT CHANGE\n",
        "\n",
        "# You are free to add your own hyperparameters as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e001635c",
      "metadata": {},
      "outputs": [],
      "source": [
        "parameters = {'n_estimators':60, 'max_depth': 5, 'min_samples_split':20}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KgpbPlH9nXDy",
      "metadata": {
        "id": "KgpbPlH9nXDy"
      },
      "source": [
        "## CONFIGURATION 1: Non-contextual Embeddings + ML Regression: 8 marks\n",
        "1 Load the non-contextual embedding model in variable `non_cont_model1`. **1 marks**\n",
        "\n",
        "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model1()` **2 marks**\n",
        "\n",
        "2 Using features as X and score as Y, train a ML based regression model (`model1`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
        "\n",
        "3 Print the correlation scores on the dev and test set predictions using trained `model1`. **1.5 mark**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "Hr7teQO9nfRR",
      "metadata": {
        "id": "Hr7teQO9nfRR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[array([-5.91304712e-03,  1.06004095e-02, -8.29132274e-03, -1.59437279e-03,\n",
            "       -2.47327867e-03, -1.61030851e-02, -1.66519335e-03,  3.23628774e-03,\n",
            "       -1.38510764e-02, -3.31928604e-03,  5.26006101e-03, -6.50755689e-03,\n",
            "        6.89097214e-03,  6.20323373e-03,  5.98087860e-03, -1.25051793e-02,\n",
            "        1.35659310e-03,  2.33549252e-03, -2.73467554e-03, -1.44716008e-02,\n",
            "       -3.72819882e-03, -3.62338917e-03,  1.27489576e-02, -1.78102066e-03,\n",
            "       -5.12126135e-03, -5.66991275e-06, -2.61300476e-03,  7.70357298e-03,\n",
            "        2.87805987e-03, -4.29937150e-03,  1.14486916e-02, -1.00336724e-03,\n",
            "        9.70217306e-03, -1.95035525e-03, -9.56572127e-03, -4.14122641e-03,\n",
            "       -6.49138121e-03, -3.88804241e-03,  3.77497473e-03,  1.55074429e-03,\n",
            "       -2.36554886e-03, -2.04946336e-05,  2.09438056e-03, -1.93349004e-03,\n",
            "        3.24052433e-03, -8.17846600e-03, -8.87624919e-03, -5.79264294e-03,\n",
            "        1.35435788e-02,  7.36733852e-03,  7.77659193e-03, -9.40764882e-03,\n",
            "       -4.04743152e-03, -4.04876424e-03, -5.52490540e-03,  1.42333126e-02,\n",
            "       -1.61141637e-04,  3.00510786e-03, -1.27660669e-02,  1.04116304e-02,\n",
            "        8.97725020e-03, -6.74671074e-03,  5.68726659e-03, -9.03321337e-03,\n",
            "       -1.52590098e-02,  4.56129480e-03, -5.91876917e-03, -1.85396802e-03,\n",
            "       -1.18807973e-02,  1.44485827e-03,  2.42990442e-03,  1.12251267e-02,\n",
            "        1.63130835e-02,  5.42069320e-03,  1.11016063e-02, -7.47503666e-03,\n",
            "        9.43609700e-03,  2.07809638e-03, -1.34843616e-02,  8.69368669e-03,\n",
            "       -8.57632235e-03, -9.76889115e-03, -5.95301436e-03, -5.89477655e-04,\n",
            "       -5.32767596e-03, -2.06726999e-03, -1.40703260e-03,  1.49080961e-03,\n",
            "        1.20989773e-02,  5.09002386e-03, -2.88483081e-03,  6.01708842e-03,\n",
            "        1.32522741e-02,  8.92570335e-03,  1.38887586e-02,  3.91898910e-03,\n",
            "       -4.38892329e-03, -9.33284033e-03, -7.85395969e-03, -4.51896712e-03],\n",
            "      dtype=float32), array([ 0.00461647,  0.07276689, -0.00690015,  0.00803107,  0.05927401,\n",
            "       -0.05487674,  0.05227448,  0.11674861, -0.0727312 , -0.02813457,\n",
            "       -0.01210483, -0.07890213, -0.00541877,  0.0152516 ,  0.02418057,\n",
            "       -0.06711631,  0.01481971,  0.02634422, -0.07622818, -0.1359049 ,\n",
            "       -0.00920207, -0.00676395,  0.0296942 , -0.07257413,  0.00647627,\n",
            "       -0.03463664, -0.00405624, -0.0098418 , -0.07207038, -0.00129334,\n",
            "        0.07044102, -0.06388691,  0.00646881, -0.04098094, -0.03411036,\n",
            "        0.0204927 ,  0.0318262 ,  0.02557642,  0.02944143, -0.07581635,\n",
            "        0.04213252, -0.02580163, -0.04512196,  0.01733257,  0.06703173,\n",
            "       -0.00107015, -0.05428937, -0.00990772,  0.04639257,  0.03429453,\n",
            "        0.01604035, -0.05401636,  0.00456569, -0.0411554 , -0.05038069,\n",
            "        0.03514578,  0.03481311, -0.05955296, -0.07994632,  0.00129185,\n",
            "       -0.00497583, -0.01736674,  0.05533154, -0.02352679, -0.06079765,\n",
            "        0.09090881,  0.02446663,  0.02080074, -0.06687244,  0.03168304,\n",
            "       -0.0239131 ,  0.01917181,  0.06338672,  0.03229543,  0.01749383,\n",
            "        0.00501966,  0.07186832,  0.00939959, -0.04188456,  0.00676676,\n",
            "       -0.03786125, -0.04356978, -0.04778106,  0.04530786, -0.00753139,\n",
            "       -0.01630466,  0.03365134, -0.01471815,  0.05089054,  0.02205407,\n",
            "        0.03525307,  0.03533912,  0.04410168,  0.01007187,  0.10453485,\n",
            "        0.0426522 ,  0.02540109, -0.03181171,  0.00506093, -0.00627899],\n",
            "      dtype=float32), array([ 9.03105550e-03,  1.18307555e-02, -1.99360144e-03,  1.00550894e-02,\n",
            "        4.51583043e-03, -7.41418079e-03, -1.95683260e-03,  6.27621356e-03,\n",
            "       -9.53625329e-03, -2.81023607e-03,  6.47676783e-03,  7.16489594e-05,\n",
            "        5.41698048e-03,  8.86563491e-03,  1.35282543e-03, -1.67662313e-03,\n",
            "       -1.13844115e-04, -8.16172175e-03, -1.34011433e-02, -5.95575618e-03,\n",
            "       -8.66994075e-03,  2.10914714e-03,  1.05707888e-02, -6.44408958e-03,\n",
            "        5.27294865e-03,  1.77207461e-03, -6.75373105e-03, -1.87662512e-03,\n",
            "       -6.61546458e-03, -5.93818724e-03,  1.63191333e-02, -7.85513874e-03,\n",
            "        1.13002388e-02, -2.30875844e-03,  6.86960435e-03, -6.76330319e-03,\n",
            "        6.52833749e-03,  1.09392237e-02,  3.81160271e-03, -1.53532065e-02,\n",
            "        5.49255824e-03,  6.77576801e-03, -3.48186726e-03, -5.64867537e-03,\n",
            "        7.62610557e-03,  5.29896095e-03, -1.34615377e-02,  5.28297387e-03,\n",
            "        1.44070908e-02,  1.29362168e-02,  8.72853864e-03,  4.04500589e-03,\n",
            "       -7.35129137e-03,  2.46149884e-03, -1.11821073e-03,  6.76914165e-03,\n",
            "       -3.06562823e-03,  3.07894312e-03, -7.35756475e-03,  1.83081499e-03,\n",
            "       -7.12247565e-03, -2.81541026e-03, -2.31920974e-03, -3.62160918e-03,\n",
            "        2.65406654e-03,  1.27010969e-02,  6.77471003e-03,  6.45409198e-03,\n",
            "       -5.37597295e-03,  1.19600706e-02, -1.24626467e-03, -2.11797119e-03,\n",
            "       -3.51909152e-03,  3.29244831e-05, -3.61414204e-05, -2.38412712e-03,\n",
            "        7.51528656e-03,  1.01457629e-03, -6.48970716e-03,  3.91956931e-03,\n",
            "        3.13780457e-03, -4.63996874e-03, -4.09092661e-03,  9.78908734e-04,\n",
            "        7.56768323e-03,  2.19350355e-03, -3.68983485e-03,  3.84682877e-04,\n",
            "        3.13220825e-03,  7.02149142e-03,  5.57714049e-03, -3.18650040e-03,\n",
            "       -3.90297710e-03, -3.97613738e-03,  8.63799732e-03,  7.62741640e-03,\n",
            "       -1.46640721e-03, -1.59362424e-03, -2.08554370e-03, -8.69713351e-03],\n",
            "      dtype=float32), array([-8.17669835e-03,  9.27992165e-03,  4.91542835e-03,  6.08345401e-03,\n",
            "        2.79336050e-03, -1.98439434e-02,  1.75980553e-02,  2.03642957e-02,\n",
            "       -1.05494801e-02,  5.94733283e-04,  8.80090799e-03, -1.07060960e-02,\n",
            "        3.65286227e-03, -2.36042077e-03,  4.80918447e-03, -7.98159372e-03,\n",
            "       -1.56812940e-03, -6.30865525e-03, -1.71227194e-02, -3.20452191e-02,\n",
            "       -8.55985936e-03, -3.25064757e-03,  1.67392008e-02, -2.06085052e-02,\n",
            "        7.60213565e-03, -2.49790843e-03, -1.03264954e-02,  4.08609660e-04,\n",
            "       -1.03633571e-03, -1.68435241e-03,  2.26823124e-03, -1.37068760e-02,\n",
            "        3.01306625e-03, -1.38555411e-02,  5.55808051e-03,  5.36774518e-03,\n",
            "        1.15633942e-02,  1.21229198e-02,  8.59260373e-03, -6.67859986e-03,\n",
            "        4.63544764e-03, -9.86164156e-03, -1.61913168e-02, -3.00519681e-03,\n",
            "        1.08106034e-02,  1.11143326e-03,  3.99313314e-04,  5.00701182e-03,\n",
            "        5.50669944e-03,  2.03165255e-04, -3.23618995e-03, -1.78506784e-03,\n",
            "        1.86205714e-03,  2.91132322e-03, -1.35705397e-02,  1.18160052e-02,\n",
            "        1.32658883e-04, -1.86959133e-02, -1.51184127e-02,  9.87530872e-03,\n",
            "       -8.47859774e-03, -1.12563595e-02,  8.61521251e-03,  1.24854536e-03,\n",
            "       -1.26555404e-02,  4.12524957e-03, -2.53166677e-03,  5.81480889e-03,\n",
            "       -5.13504317e-04, -3.26243672e-03, -1.53248142e-02,  1.10326558e-02,\n",
            "        1.27701797e-02, -3.29888635e-03,  6.30283542e-03, -5.34056104e-04,\n",
            "        1.15134129e-02,  5.26062725e-03, -1.42472377e-02,  1.00618601e-02,\n",
            "       -5.08996099e-03, -4.92143305e-03, -8.66542757e-03,  9.44294152e-06,\n",
            "        5.84222376e-03, -5.19307656e-03, -2.85116793e-03,  6.09246548e-03,\n",
            "        1.56781555e-03, -2.05532857e-03, -2.93220230e-03, -6.74404437e-04,\n",
            "        8.39958340e-03,  1.76679646e-03,  2.36178171e-02,  4.16423939e-03,\n",
            "        6.79320889e-03,  9.03574284e-04, -1.58430892e-03,  9.63009614e-03],\n",
            "      dtype=float32)]\n",
            "0.4684569089469468\n",
            "SpearmanrResult(correlation=-0.04596965892222895, pvalue=0.07509986823123005)\n",
            "SpearmanrResult(correlation=-0.02260901551024108, pvalue=0.40150906504374195)\n"
          ]
        }
      ],
      "source": [
        "def get_feature_model1(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the embedding vectors for the each sentence column using non_cont_model1,\n",
        "  Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "  \"\"\"\n",
        "  # print(data_frame['se'])\n",
        "  a = []\n",
        "  for i in data_frame['sent_a']:\n",
        "    a.append(gensim.utils.simple_preprocess(i))\n",
        "  # print(a)\n",
        "  b = []\n",
        "  for i in data_frame['sent_b']:\n",
        "    b.append(gensim.utils.simple_preprocess(i))\n",
        "    \n",
        "  non_cont_model1.build_vocab(a)\n",
        "  non_cont_model1.train(a, total_examples=non_cont_model1.corpus_count, epochs=NUM_EPOCHS, report_delay=1)\n",
        "  # len(non_cont_model1.wv.vocab.keys())\n",
        "  \n",
        "  emb_a = []\n",
        "  emb_b = []\n",
        "  \n",
        "  for i in range(len(a)):\n",
        "    sentences_a = []\n",
        "    sentences_b = []\n",
        "    \n",
        "    for j in range(len(a[i])):\n",
        "      sentences_a.append(non_cont_model1.wv[a[i][j]])\n",
        "      \n",
        "    for k in range(len(a[i])):\n",
        "      sentences_b.append(non_cont_model1.wv[a[i][k]])\n",
        "      \n",
        "    emb_a.append(sentences_a)\n",
        "    emb_b.append(sentences_b)\n",
        "    \n",
        "    # print(emb_a)\n",
        "  \n",
        "  return emb_a,emb_b\n",
        "  \n",
        "def get_average_array(array_to_avg):\n",
        "  ret_array = []\n",
        "\n",
        "  for i in range(len(array_to_avg)):\n",
        "    ret_array.append(np.average(array_to_avg[i]))\n",
        "\n",
        "  return ret_array\n",
        "\n",
        "non_cont_model1 = Word2Vec(min_count=1, window=2, workers=cores-1)\n",
        "\n",
        "\n",
        "feature_1_train, feature_2_train = get_feature_model1(df_train)\n",
        "feature_1_dev, feature_2_dev = get_feature_model1(df_dev)\n",
        "feature_1_test, feature_2_test = get_feature_model1(df_test)\n",
        "\n",
        "# print(feature_1_train[0])\n",
        "\n",
        "feature_1_train_avg = get_average_array(feature_1_train)\n",
        "feature_2_train_avg = get_average_array(feature_2_train)\n",
        "\n",
        "feature_1_dev_avg = get_average_array(feature_1_dev)\n",
        "feature_2_dev_avg = get_average_array(feature_2_dev)\n",
        "\n",
        "feature_1_test_avg = get_average_array(feature_1_test)\n",
        "feature_2_test_avg = get_average_array(feature_2_test)\n",
        "\n",
        "X_train = np.column_stack((feature_1_train_avg,feature_2_train_avg))\n",
        "Y_train = np.array(df_train['score'])\n",
        "\n",
        "X_dev = np.column_stack((feature_1_dev_avg,feature_2_dev_avg))\n",
        "Y_dev = df_dev.score\n",
        "\n",
        "X_test = np.column_stack((feature_1_test_avg,feature_2_test_avg))\n",
        "Y_test = df_test.score\n",
        "# Initiate a regression model and train it.\n",
        "\n",
        "reg = RandomForestRegressor()\n",
        "reg.fit(X_train, Y_train)\n",
        "print(reg.score(X_train, Y_train))\n",
        "\n",
        "print(spearmanr(reg.predict(X_dev), Y_dev))\n",
        "print(spearmanr(reg.predict(X_test), Y_test))\n",
        "\n",
        "\n",
        "# print(reg)\n",
        "# Print spearmanr correlation on the predicted output of the dev and test sets.\n",
        "# print(feature_2_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DBzjbQ-grL8H",
      "metadata": {
        "id": "DBzjbQ-grL8H"
      },
      "source": [
        "## CONFIGURATION 2: Contextual Embeddings + ML Regression: 7 marks\n",
        "1 Load the contextual embedding model in variable `non_cont_model2`. **1 marks**\n",
        "\n",
        "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model2()` **2 marks**\n",
        "\n",
        "2 Using features as X and score as Y, train a ML based regression model (`model2`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
        "\n",
        "3 Print the correlation scores on the dev and test set predictions using trained `model2`. **1.5 mark**\n",
        "\n",
        "Useful references: https://www.sbert.net/docs/usage/semantic_textual_similarity.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GlTVNjv0sNP0",
      "metadata": {
        "id": "GlTVNjv0sNP0"
      },
      "outputs": [],
      "source": [
        "def get_feature_model2(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the embedding vectors for the each sentence column using model2,\n",
        "  Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "  \"\"\"\n",
        "\n",
        "# non_cont_model2 = \n",
        "\n",
        "# feature_1_<dataset_type>, feature_2_<dataset_type> = get_feature_model2(data_frame)\n",
        "# X_<dataset_type>, Y_<dataset_type> = \n",
        "# Initiate a regression model and train it.\n",
        "# Print spearman correlation on the predicted output of the dev and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VImljTWps_GR",
      "metadata": {
        "id": "VImljTWps_GR"
      },
      "source": [
        "## CONFIGURATION 3: Fine-Tune a Contextual Embeddings Model: 18 marks\n",
        "1 Prepare data samples to be for the DL model to consume. Add the code in the `form_data()`. **4 marks**\n",
        "\n",
        "3 Create the data loader, one each for train/dev/test data_input sample set obtained from `form_input_example()`. **1.5 marks**\n",
        "\n",
        "4 Initiate `model3` consisting of **atleast** the following 3 components - `base_LM`, a `pooling_layer` and a `dense_layer`. Use appropriate activation function in dense. **Atleast** one layer of `base_LM` should be set to trainable. **5 marks**\n",
        "\n",
        "6 Initiate the `loss`. **0.5 marks**\n",
        "\n",
        "7 Fit the `model3`. Use `NUM_EPOCHS = 2`. **MAX_NUM_EPOCHS allowed will be 3**. **2 marks** \n",
        "\n",
        "8 Complete the `get_model_predicts()` to obtain predicted scores for input sentence pairs. **3.5 marks** \n",
        "\n",
        "9 Print the correlation scores on the dev and test set predictions. **1.5 mark**\n",
        "\n",
        "Useful References: https://huggingface.co/blog/how-to-train-sentence-transformers "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0kb0xJZmZGIR",
      "metadata": {
        "id": "0kb0xJZmZGIR"
      },
      "outputs": [],
      "source": [
        "def form_data(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the dataloder.\n",
        "  \"\"\"\n",
        "\n",
        "def get_model_predicts(data_type, trained_model):\n",
        "  \"\"\"\n",
        "  Input the dataset list and return a list of cosine similarity scores. Use the fitted final_trainable_model for obtaining encodings.\n",
        "  \"\"\"\n",
        "\n",
        "# dataloader_<dataset_type> = form_data(data_frame)\n",
        "# base_model = \n",
        "# layer_ppoling = \n",
        "# layer_dense = \n",
        "# model3 = \n",
        "# loss =\n",
        "\n",
        "# Fit the model3.\n",
        "# Print spearman correlation on the predicted output of the dev and test sets."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "d67b38e366ca74dac447d11a031a36b4e680cd6b313aec980c02ee663b193322"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
