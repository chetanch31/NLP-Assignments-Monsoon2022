{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "PpcoqA-3l2g9",
      "metadata": {
        "id": "PpcoqA-3l2g9"
      },
      "source": [
        "## Installing Sentence Transsformer and other models/frameworks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "0tw0xe31hT59",
      "metadata": {
        "id": "0tw0xe31hT59"
      },
      "outputs": [],
      "source": [
        "# pip install -U sentence-transformers\n",
        "# pip install gensim\n",
        "\n",
        "\n",
        "# Kindly add all your installations and versions if any in this cell."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WWWTQtXDl-t8",
      "metadata": {
        "id": "WWWTQtXDl-t8"
      },
      "source": [
        "## Importing necessary libraries. \n",
        "In the final version all imports should be stricly enlisted here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "e951f9be",
      "metadata": {
        "id": "e951f9be"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import multiprocessing\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "import gensim\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# import spacy\n",
        "import scipy\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, losses, models, util\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers.readers import InputExample\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "# import torch \n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H5Ch9I58mMGe",
      "metadata": {
        "id": "H5Ch9I58mMGe"
      },
      "source": [
        "## Load dataset: 7 marks\n",
        "1 Download and unzip the dataset from this link http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz  **1 mark**\n",
        "\n",
        "2 Complete the code in `read_sts_csv()`. **4.5 marks**\n",
        "\n",
        "3 Create 3 dataframes one each for train, test and val and print their final shapes. **1.5 marks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2TMR0Z0DlfFf",
      "metadata": {
        "id": "2TMR0Z0DlfFf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev: (1500, 7)\n",
            "train: (5549, 7)\n",
            "test: (1379, 7)\n"
          ]
        }
      ],
      "source": [
        "# For preprocessing in sts-train.csv, we removed \"Europe Media Monitor (http://emm.newsbrief.eu)\" from the entire csv file\n",
        "# It reduced the number of buggy lines to less than 10 from more than 200.\n",
        "\n",
        "# For preprocessing in sts-dev.csv, we removed \"Europe Media Monitor (http://emm.newsbrief.eu)\" from the entire csv file\n",
        "# It reduced the number of buggy lines to less than 10 from more than 40.\n",
        "\n",
        "# For preprocessing in sts-test.csv, we removed \"Europe Media Monitor (http://emm.newsbrief.eu)\" from the entire csv file along with all mentions of stack exchange links.\n",
        "# It reduced the number of buggy lines from more than 300 to close to 0.\n",
        "\n",
        "def read_sts_csv(dataset_type=\"dev\", columns=['source', 'type', 'year', 'id', 'score', 'sent_a', 'sent_b']):\n",
        "  path = INPUT_PATH + \"/sts-\"+ dataset_type + \".csv\"\n",
        "  df = pd.read_csv(path, sep='\\t', header=None, on_bad_lines = 'skip',lineterminator='\\n',quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
        "  df.columns = columns\n",
        "  # df.describe()\n",
        "  return df\n",
        "  \n",
        "INPUT_PATH = r\"stsbenchmark\"\n",
        "df_dev = read_sts_csv(\"dev\") # create the train, dev and test dataframes\n",
        "df_train = read_sts_csv(\"train\")\n",
        "df_test = read_sts_csv(\"test\")\n",
        "df_train = df_train.dropna()\n",
        "print(\"dev:\",df_dev.shape)\n",
        "print(\"train:\",df_train.shape)\n",
        "print(\"test:\",df_test.shape)\n",
        "# print(df_train['sent_a'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gO2ZkIwDmo4s",
      "metadata": {
        "id": "gO2ZkIwDmo4s"
      },
      "source": [
        "## Hyperparameters: 5 Marks\n",
        "Update this cell with you choosen parameters except, NUM_EPOCHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5b3dddd6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n"
          ]
        }
      ],
      "source": [
        "cores = multiprocessing.cpu_count() \n",
        "print(cores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4QurhOG7E0Z-",
      "metadata": {
        "id": "4QurhOG7E0Z-"
      },
      "outputs": [],
      "source": [
        "\n",
        "HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL ='bert-base-uncased' # USE THE HUGGAING FACE VERSION OF SENTENCE_TRANSFORMER_TYPE\n",
        "NON_CONEXTUAL_MODEL_TYPE = 'Word2Vec'\n",
        "CONEXTUAL_MODEL_TYPE = 'sentence-transformers/all-mpnet-base-v2'\n",
        "INPUT_PATH = r\"stsbenchmark\"\n",
        "BATCH_SIZE = 1024\n",
        "OUT_DIM_DENSE = 5\n",
        "NUM_EPOCHS = 2 ## THIS IS FIXED DO NOT CHANGE\n",
        "\n",
        "# You are free to add your own hyperparameters as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "e001635c",
      "metadata": {},
      "outputs": [],
      "source": [
        "parameters = {'n_estimators':80, 'max_depth': 15, 'min_samples_split':15}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KgpbPlH9nXDy",
      "metadata": {
        "id": "KgpbPlH9nXDy"
      },
      "source": [
        "## CONFIGURATION 1: Non-contextual Embeddings + ML Regression: 8 marks\n",
        "1 Load the non-contextual embedding model in variable `non_cont_model1`. **1 marks**\n",
        "\n",
        "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model1()` **2 marks**\n",
        "\n",
        "2 Using features as X and score as Y, train a ML based regression model (`model1`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
        "\n",
        "3 Print the correlation scores on the dev and test set predictions using trained `model1`. **1.5 mark**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "Hr7teQO9nfRR",
      "metadata": {
        "id": "Hr7teQO9nfRR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.15306961701540678\n",
            "0.15078729068076132\n",
            "0.21972810153745807\n"
          ]
        }
      ],
      "source": [
        "def get_feature_model1(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the embedding vectors for the each sentence column using non_cont_model1,\n",
        "  Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "  \"\"\"\n",
        "  # print(data_frame['se'])\n",
        "  a = []\n",
        "  for i in data_frame['sent_a']:\n",
        "    a.append(gensim.utils.simple_preprocess(i))\n",
        "  # print(a)\n",
        "  b = []\n",
        "  for i in data_frame['sent_b']:\n",
        "    b.append(gensim.utils.simple_preprocess(i))\n",
        "    \n",
        "  non_cont_model1.build_vocab(a)\n",
        "  non_cont_model1.train(a, total_examples=non_cont_model1.corpus_count, epochs=NUM_EPOCHS, report_delay=1)\n",
        "  # len(non_cont_model1.wv.vocab.keys())\n",
        "  \n",
        "  emb_a = []\n",
        "  emb_b = []\n",
        "  mm = 0\n",
        "  \n",
        "  for i in range(len(a)):\n",
        "    sentences_a = []\n",
        "    sentences_b = []\n",
        "    \n",
        "    for j in range(len(a[i])):\n",
        "      sentences_a.append(non_cont_model1.wv[a[i][j]])\n",
        "      if mm == 0:\n",
        "        # print(non_cont_model1.wv[a[i][j]])\n",
        "        mm=+1\n",
        "      \n",
        "    for k in range(len(a[i])):\n",
        "      sentences_b.append(non_cont_model1.wv[a[i][k]])\n",
        "      \n",
        "    emb_a.append(sentences_a)\n",
        "    emb_b.append(sentences_b)\n",
        "    \n",
        "    # print(emb_a)\n",
        "  \n",
        "  return emb_a,emb_b\n",
        "  \n",
        "def get_average_array(array_to_avg):\n",
        "  ret_array = []\n",
        "\n",
        "  for i in range(len(array_to_avg)):\n",
        "    ret_array.append(np.average(array_to_avg[i]))\n",
        "\n",
        "  return ret_array\n",
        "\n",
        "non_cont_model1 = Word2Vec(vector_size = 21,min_count=1, window=1, workers=cores-1)\n",
        "\n",
        "\n",
        "feature_1_train, feature_2_train = get_feature_model1(df_train)\n",
        "feature_1_dev, feature_2_dev = get_feature_model1(df_dev)\n",
        "feature_1_test, feature_2_test = get_feature_model1(df_test)\n",
        "\n",
        "# print(feature_1_train[0])\n",
        "\n",
        "feature_1_train_avg = get_average_array(feature_1_train)\n",
        "feature_2_train_avg = get_average_array(feature_2_train)\n",
        "\n",
        "feature_1_dev_avg = get_average_array(feature_1_dev)\n",
        "feature_2_dev_avg = get_average_array(feature_2_dev)\n",
        "\n",
        "feature_1_test_avg = get_average_array(feature_1_test)\n",
        "feature_2_test_avg = get_average_array(feature_2_test)\n",
        "\n",
        "X_train = np.column_stack((feature_1_train_avg,feature_2_train_avg))\n",
        "Y_train = np.array(df_train['score'])\n",
        "\n",
        "X_dev = np.column_stack((feature_1_dev_avg,feature_2_dev_avg))\n",
        "Y_dev = df_dev.score\n",
        "\n",
        "X_test = np.column_stack((feature_1_test_avg,feature_2_test_avg))\n",
        "Y_test = df_test.score\n",
        "# Initiate a regression model and train it.\n",
        "\n",
        "# reg = RandomForestRegressor(**parameters)\n",
        "# reg.fit(X_train, Y_train)\n",
        "print(reg.score(X_train, Y_train))\n",
        "\n",
        "print(spearmanr(reg.predict(X_dev), Y_dev).correlation)\n",
        "print(spearmanr(reg.predict(X_test), Y_test).correlation)\n",
        "\n",
        "# print(reg)\n",
        "# Print spearmanr correlation on the predicted output of the dev and test sets.\n",
        "# print(feature_2_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DBzjbQ-grL8H",
      "metadata": {
        "id": "DBzjbQ-grL8H"
      },
      "source": [
        "## CONFIGURATION 2: Contextual Embeddings + ML Regression: 7 marks\n",
        "1 Load the contextual embedding model in variable `non_cont_model2`. **1 marks**\n",
        "\n",
        "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model2()` **2 marks**\n",
        "\n",
        "2 Using features as X and score as Y, train a ML based regression model (`model2`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
        "\n",
        "3 Print the correlation scores on the dev and test set predictions using trained `model2`. **1.5 mark**\n",
        "\n",
        "Useful references: https://www.sbert.net/docs/usage/semantic_textual_similarity.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "GlTVNjv0sNP0",
      "metadata": {
        "id": "GlTVNjv0sNP0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8993502746932975\n",
            "SpearmanrResult(correlation=0.6227549570137937, pvalue=7.759377489794083e-162)\n",
            "SpearmanrResult(correlation=0.6044111676694474, pvalue=4.100804715813971e-138)\n"
          ]
        }
      ],
      "source": [
        "def get_feature_model2(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the embedding vectors for the each sentence column using model2,\n",
        "  Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "  \"\"\"\n",
        "  a = data_frame['sent_a']\n",
        "  b = data_frame['sent_b']\n",
        "  \n",
        "  emb_a = non_cont_model2.encode(a)\n",
        "  emb_b = non_cont_model2.encode(b)\n",
        "  \n",
        "  return emb_a, emb_b\n",
        "  \n",
        "\n",
        "non_cont_model2 = SentenceTransformer(CONEXTUAL_MODEL_TYPE)\n",
        "\n",
        "feature_1_train2, feature_2_train2 = get_feature_model2(df_train)\n",
        "feature_1_dev2, feature_2_dev2 = get_feature_model2(df_dev)\n",
        "feature_1_test2, feature_2_test2 = get_feature_model2(df_test)\n",
        "\n",
        "# feature_1_<dataset_type>, feature_2_<dataset_type> = get_feature_model2(data_frame)\n",
        "# X_<dataset_type>, Y_<dataset_type> = \n",
        "# Initiate a regression model and train it.\n",
        "# Print spearman correlation on the predicted output of the dev and test sets.\n",
        "X_train2 = np.column_stack((feature_1_train2,feature_2_train2))\n",
        "Y_train2 = np.array(df_train['score'])\n",
        "\n",
        "X_dev2 = np.column_stack((feature_1_dev2,feature_2_dev2))\n",
        "Y_dev2 = df_dev.score\n",
        "\n",
        "X_test2 = np.column_stack((feature_1_test2,feature_2_test2))\n",
        "Y_test2 = df_test.score\n",
        "\n",
        "reg2 = RandomForestRegressor()\n",
        "reg2.fit(X_train2, Y_train2)\n",
        "print(reg2.score(X_train2, Y_train2))\n",
        "\n",
        "print(spearmanr(reg2.predict(X_dev2), Y_dev2))\n",
        "print(spearmanr(reg2.predict(X_test2), Y_test2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VImljTWps_GR",
      "metadata": {
        "id": "VImljTWps_GR"
      },
      "source": [
        "## CONFIGURATION 3: Fine-Tune a Contextual Embeddings Model: 18 marks\n",
        "1 Prepare data samples to be for the DL model to consume. Add the code in the `form_data()`. **4 marks**\n",
        "\n",
        "3 Create the data loader, one each for train/dev/test data_input sample set obtained from `form_input_example()`. **1.5 marks**\n",
        "\n",
        "4 Initiate `model3` consisting of **atleast** the following 3 components - `base_LM`, a `pooling_layer` and a `dense_layer`. Use appropriate activation function in dense. **Atleast** one layer of `base_LM` should be set to trainable. **5 marks**\n",
        "\n",
        "6 Initiate the `loss`. **0.5 marks**\n",
        "\n",
        "7 Fit the `model3`. Use `NUM_EPOCHS = 2`. **MAX_NUM_EPOCHS allowed will be 3**. **2 marks** \n",
        "\n",
        "8 Complete the `get_model_predicts()` to obtain predicted scores for input sentence pairs. **3.5 marks** \n",
        "\n",
        "9 Print the correlation scores on the dev and test set predictions. **1.5 mark**\n",
        "\n",
        "Useful References: https://huggingface.co/blog/how-to-train-sentence-transformers "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0kb0xJZmZGIR",
      "metadata": {
        "id": "0kb0xJZmZGIR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Iteration: 100%|██████████| 347/347 [20:01<00:00,  3.46s/it]\n",
            "Iteration: 100%|██████████| 347/347 [20:13<00:00,  3.50s/it]\n",
            "Epoch: 100%|██████████| 2/2 [40:15<00:00, 1207.58s/it]\n"
          ]
        }
      ],
      "source": [
        "def form_data(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the dataloder.\n",
        "  \"\"\"\n",
        "  train_ex = []\n",
        "\n",
        "  for i in range(len(data_frame)):\n",
        "    train_ex.append(InputExample(texts=[data_frame['sent_a'][i], data_frame['sent_b'][i]], label=float(data_frame['score'][i])))\n",
        "  \n",
        "  loader = DataLoader(train_ex, shuffle=True, batch_size=16)\n",
        "  return loader\n",
        "  \n",
        "\n",
        "# dataloader_<dataset_type> = form_data(data_frame)\n",
        "a = df_dev['sent_a']\n",
        "b = df_dev['sent_b']\n",
        "s = df_dev['score']\n",
        "\n",
        "evaluator = EmbeddingSimilarityEvaluator(a,b,s)\n",
        "\n",
        "# dev_set = form_data(df_dev)\n",
        "# test_set = form_data(df_test)\n",
        "train_set = form_data(df_train)\n",
        "\n",
        "word_model_embedding = models.Transformer('bert-base-uncased', max_seq_length=256)\n",
        "layer_pooling = models.Pooling(word_model_embedding.get_word_embedding_dimension(), pooling_mode='max')\n",
        "layer_dense = models.Dense(in_features=layer_pooling.get_sentence_embedding_dimension(), out_features=OUT_DIM_DENSE)\n",
        "model3 = SentenceTransformer(modules=[word_model_embedding, layer_pooling, layer_dense])\n",
        "\n",
        "loss = losses.CosineSimilarityLoss(model=model3)\n",
        "\n",
        "warmup_steps = int(len(train_set) * NUM_EPOCHS * 0.1)\n",
        "# # Fit the model3.\n",
        "model3.fit(train_objectives=[(train_set, loss)], epochs=NUM_EPOCHS,warmup_steps=warmup_steps, evaluator=evaluator, evaluation_steps=300) \n",
        "\n",
        "# # Print spearman correlation on the predicted output of the dev and test sets.\n",
        "# y_train3 = get_model_predicts(df_train, model3)\n",
        "\n",
        "# X_dev3 = np.column_stack((feature_1_dev2,feature_2_dev2))\n",
        "# Y_dev3 = df_dev.score\n",
        "\n",
        "# X_test3 = np.column_stack((feature_1_test2,feature_2_test2))\n",
        "# Y_test3 = df_test.score\n",
        "\n",
        "# print(spearmanr(reg2.predict(X_dev2), Y_dev2))\n",
        "# print(spearmanr(reg2.predict(X_test2), Y_test2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "4ba8e097",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7087135795647355\n",
            "0.6987652747833221\n"
          ]
        }
      ],
      "source": [
        "def get_model_predicts(data_type, trained_model):\n",
        "  \"\"\"\n",
        "  Input the dataset list and return a list of cosine similarity scores. Use the fitted final_trainable_model for obtaining encodings.\n",
        "  \"\"\"\n",
        "  a = data_type['sent_a']\n",
        "  b = data_type['sent_b']\n",
        "  emb_a=trained_model.encode(a)\n",
        "  emb_b=trained_model.encode(b)\n",
        "  list_1 = []\n",
        "  for i in range(len(a)):\n",
        "    emb_a=trained_model.encode(a[i])\n",
        "    emb_b=trained_model.encode(b[i])\n",
        "  # print\n",
        "    # emb_a.reshape(1, -1)\n",
        "    # emb_b.reshape(1, -1)\n",
        "    list_1.append(1-scipy.spatial.distance.cosine(emb_a,emb_b))\n",
        "  # list_1 = cosine_similarity(emb_a, emb_b, dense_output=True)\n",
        "  return list_1\n",
        "\n",
        "test_results = get_model_predicts(df_test,model3)\n",
        "dev_results = get_model_predicts(df_dev,model3)\n",
        "# print(test_results)\n",
        "# type(test_results)\n",
        "# print(np.average(spearmanr(test_results,df_test['score']).correlation))\n",
        "print(spearmanr(test_results,df_test['score']).correlation)\n",
        "print(spearmanr(dev_results,df_dev['score']).correlation)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "cbab7ad93b4c816b81ea2fcadccb708733fafe3d4de9be705110c0acce2d8094"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
