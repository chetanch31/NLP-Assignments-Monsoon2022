{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "PpcoqA-3l2g9",
      "metadata": {
        "id": "PpcoqA-3l2g9"
      },
      "source": [
        "## Installing Sentence Transsformer and other models/frameworks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "0tw0xe31hT59",
      "metadata": {
        "id": "0tw0xe31hT59"
      },
      "outputs": [],
      "source": [
        "# pip install -U sentence-transformers\n",
        "# pip install gensim\n",
        "\n",
        "\n",
        "# Kindly add all your installations and versions if any in this cell."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WWWTQtXDl-t8",
      "metadata": {
        "id": "WWWTQtXDl-t8"
      },
      "source": [
        "## Importing necessary libraries. \n",
        "In the final version all imports should be stricly enlisted here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "e951f9be",
      "metadata": {
        "id": "e951f9be"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import multiprocessing\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "import gensim\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# import spacy\n",
        "# from scipy import stats\n",
        "from sklearn import linear_model\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, losses, models, util\n",
        "# from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "# from sentence_transformers.readers import InputExample\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "# import torch \n",
        "# from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H5Ch9I58mMGe",
      "metadata": {
        "id": "H5Ch9I58mMGe"
      },
      "source": [
        "## Load dataset: 7 marks\n",
        "1 Download and unzip the dataset from this link http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz  **1 mark**\n",
        "\n",
        "2 Complete the code in `read_sts_csv()`. **4.5 marks**\n",
        "\n",
        "3 Create 3 dataframes one each for train, test and val and print their final shapes. **1.5 marks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "2TMR0Z0DlfFf",
      "metadata": {
        "id": "2TMR0Z0DlfFf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev: (1500, 7)\n",
            "train: (5549, 7)\n",
            "test: (1379, 7)\n"
          ]
        }
      ],
      "source": [
        "# For preprocessing in sts-train.csv, we removed \"Europe Media Monitor (http://emm.newsbrief.eu)\" from the entire csv file\n",
        "# It reduced the number of buggy lines to less than 10 from more than 200.\n",
        "\n",
        "# For preprocessing in sts-dev.csv, we removed \"Europe Media Monitor (http://emm.newsbrief.eu)\" from the entire csv file\n",
        "# It reduced the number of buggy lines to less than 10 from more than 40.\n",
        "\n",
        "# For preprocessing in sts-test.csv, we removed \"Europe Media Monitor (http://emm.newsbrief.eu)\" from the entire csv file along with all mentions of stack exchange links.\n",
        "# It reduced the number of buggy lines from more than 300 to close to 0.\n",
        "\n",
        "def read_sts_csv(dataset_type=\"dev\", columns=['source', 'type', 'year', 'id', 'score', 'sent_a', 'sent_b']):\n",
        "  path = INPUT_PATH + \"/sts-\"+ dataset_type + \".csv\"\n",
        "  df = pd.read_csv(path, sep='\\t', header=None, on_bad_lines = 'skip',lineterminator='\\n',quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
        "  df.columns = columns\n",
        "  # df.describe()\n",
        "  return df\n",
        "  \n",
        "INPUT_PATH = r\"stsbenchmark\"\n",
        "df_dev = read_sts_csv(\"dev\") # create the train, dev and test dataframes\n",
        "df_train = read_sts_csv(\"train\")\n",
        "df_test = read_sts_csv(\"test\")\n",
        "df_train = df_train.dropna()\n",
        "print(\"dev:\",df_dev.shape)\n",
        "print(\"train:\",df_train.shape)\n",
        "print(\"test:\",df_test.shape)\n",
        "# print(df_train['sent_a'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gO2ZkIwDmo4s",
      "metadata": {
        "id": "gO2ZkIwDmo4s"
      },
      "source": [
        "## Hyperparameters: 5 Marks\n",
        "Update this cell with you choosen parameters except, NUM_EPOCHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "5b3dddd6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n"
          ]
        }
      ],
      "source": [
        "cores = multiprocessing.cpu_count() \n",
        "print(cores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "4QurhOG7E0Z-",
      "metadata": {
        "id": "4QurhOG7E0Z-"
      },
      "outputs": [],
      "source": [
        "\n",
        "HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL ='sentence-transformers/all-mpnet-base-v2' # USE THE HUGGAING FACE VERSION OF SENTENCE_TRANSFORMER_TYPE\n",
        "NON_CONEXTUAL_MODEL_TYPE = 'Word2Vec'\n",
        "CONEXTUAL_MODEL_TYPE = HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL\n",
        "INPUT_PATH = r\"stsbenchmark\"\n",
        "BATCH_SIZE = 1024\n",
        "OUT_DIM_DENSE = 128\n",
        "NUM_EPOCHS = 2 ## THIS IS FIXED DO NOT CHANGE\n",
        "\n",
        "# You are free to add your own hyperparameters as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "e001635c",
      "metadata": {},
      "outputs": [],
      "source": [
        "parameters = {'n_estimators':60, 'max_depth': 5, 'min_samples_split':20}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KgpbPlH9nXDy",
      "metadata": {
        "id": "KgpbPlH9nXDy"
      },
      "source": [
        "## CONFIGURATION 1: Non-contextual Embeddings + ML Regression: 8 marks\n",
        "1 Load the non-contextual embedding model in variable `non_cont_model1`. **1 marks**\n",
        "\n",
        "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model1()` **2 marks**\n",
        "\n",
        "2 Using features as X and score as Y, train a ML based regression model (`model1`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
        "\n",
        "3 Print the correlation scores on the dev and test set predictions using trained `model1`. **1.5 mark**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "Hr7teQO9nfRR",
      "metadata": {
        "id": "Hr7teQO9nfRR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5042613921948844\n",
            "SpearmanrResult(correlation=-0.0586395598115972, pvalue=0.02313720013113114)\n",
            "SpearmanrResult(correlation=-0.035959691124210845, pvalue=0.18201390445583188)\n"
          ]
        }
      ],
      "source": [
        "def get_feature_model1(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the embedding vectors for the each sentence column using non_cont_model1,\n",
        "  Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "  \"\"\"\n",
        "  # print(data_frame['se'])\n",
        "  a = []\n",
        "  for i in data_frame['sent_a']:\n",
        "    a.append(gensim.utils.simple_preprocess(i))\n",
        "  # print(a)\n",
        "  b = []\n",
        "  for i in data_frame['sent_b']:\n",
        "    b.append(gensim.utils.simple_preprocess(i))\n",
        "    \n",
        "  non_cont_model1.build_vocab(a)\n",
        "  non_cont_model1.train(a, total_examples=non_cont_model1.corpus_count, epochs=NUM_EPOCHS, report_delay=1)\n",
        "  # len(non_cont_model1.wv.vocab.keys())\n",
        "  \n",
        "  emb_a = []\n",
        "  emb_b = []\n",
        "  \n",
        "  for i in range(len(a)):\n",
        "    sentences_a = []\n",
        "    sentences_b = []\n",
        "    \n",
        "    for j in range(len(a[i])):\n",
        "      sentences_a.append(non_cont_model1.wv[a[i][j]])\n",
        "      \n",
        "    for k in range(len(a[i])):\n",
        "      sentences_b.append(non_cont_model1.wv[a[i][k]])\n",
        "      \n",
        "    emb_a.append(sentences_a)\n",
        "    emb_b.append(sentences_b)\n",
        "    \n",
        "    # print(emb_a)\n",
        "  \n",
        "  return emb_a,emb_b\n",
        "  \n",
        "def get_average_array(array_to_avg):\n",
        "  ret_array = []\n",
        "\n",
        "  for i in range(len(array_to_avg)):\n",
        "    ret_array.append(np.average(array_to_avg[i]))\n",
        "\n",
        "  return ret_array\n",
        "\n",
        "non_cont_model1 = Word2Vec(min_count=1, window=2, workers=cores-1)\n",
        "\n",
        "\n",
        "feature_1_train, feature_2_train = get_feature_model1(df_train)\n",
        "feature_1_dev, feature_2_dev = get_feature_model1(df_dev)\n",
        "feature_1_test, feature_2_test = get_feature_model1(df_test)\n",
        "\n",
        "# print(feature_1_train[0])\n",
        "\n",
        "feature_1_train_avg = get_average_array(feature_1_train)\n",
        "feature_2_train_avg = get_average_array(feature_2_train)\n",
        "\n",
        "feature_1_dev_avg = get_average_array(feature_1_dev)\n",
        "feature_2_dev_avg = get_average_array(feature_2_dev)\n",
        "\n",
        "feature_1_test_avg = get_average_array(feature_1_test)\n",
        "feature_2_test_avg = get_average_array(feature_2_test)\n",
        "\n",
        "X_train = np.column_stack((feature_1_train_avg,feature_2_train_avg))\n",
        "Y_train = np.array(df_train['score'])\n",
        "\n",
        "X_dev = np.column_stack((feature_1_dev_avg,feature_2_dev_avg))\n",
        "Y_dev = df_dev.score\n",
        "\n",
        "X_test = np.column_stack((feature_1_test_avg,feature_2_test_avg))\n",
        "Y_test = df_test.score\n",
        "# Initiate a regression model and train it.\n",
        "\n",
        "reg = RandomForestRegressor()\n",
        "reg.fit(X_train, Y_train)\n",
        "print(reg.score(X_train, Y_train))\n",
        "\n",
        "print(spearmanr(reg.predict(X_dev), Y_dev))\n",
        "print(spearmanr(reg.predict(X_test), Y_test))\n",
        "\n",
        "\n",
        "# print(reg)\n",
        "# Print spearmanr correlation on the predicted output of the dev and test sets.\n",
        "# print(feature_2_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DBzjbQ-grL8H",
      "metadata": {
        "id": "DBzjbQ-grL8H"
      },
      "source": [
        "## CONFIGURATION 2: Contextual Embeddings + ML Regression: 7 marks\n",
        "1 Load the contextual embedding model in variable `non_cont_model2`. **1 marks**\n",
        "\n",
        "2 Get feature for the sentences using the LM model loaded before. Add the code in the `get_feature_model2()` **2 marks**\n",
        "\n",
        "2 Using features as X and score as Y, train a ML based regression model (`model2`). You are free to choose any sklearn based regression method, and its hyperparameters. **3.5 marks**\n",
        "\n",
        "3 Print the correlation scores on the dev and test set predictions using trained `model2`. **1.5 mark**\n",
        "\n",
        "Useful references: https://www.sbert.net/docs/usage/semantic_textual_similarity.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "GlTVNjv0sNP0",
      "metadata": {
        "id": "GlTVNjv0sNP0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8993502746932975\n",
            "SpearmanrResult(correlation=0.6227549570137937, pvalue=7.759377489794083e-162)\n",
            "SpearmanrResult(correlation=0.6044111676694474, pvalue=4.100804715813971e-138)\n"
          ]
        }
      ],
      "source": [
        "def get_feature_model2(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the embedding vectors for the each sentence column using model2,\n",
        "  Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "  \"\"\"\n",
        "  a = data_frame['sent_a']\n",
        "  b = data_frame['sent_b']\n",
        "  \n",
        "  emb_a = non_cont_model2.encode(a)\n",
        "  emb_b = non_cont_model2.encode(b)\n",
        "  \n",
        "  return emb_a, emb_b\n",
        "  \n",
        "\n",
        "non_cont_model2 = SentenceTransformer(HUGGING_FACE_SENTENCE_TRANSFORMER_MODEL)\n",
        "\n",
        "feature_1_train2, feature_2_train2 = get_feature_model2(df_train)\n",
        "feature_1_dev2, feature_2_dev2 = get_feature_model2(df_dev)\n",
        "feature_1_test2, feature_2_test2 = get_feature_model2(df_test)\n",
        "\n",
        "# feature_1_<dataset_type>, feature_2_<dataset_type> = get_feature_model2(data_frame)\n",
        "# X_<dataset_type>, Y_<dataset_type> = \n",
        "# Initiate a regression model and train it.\n",
        "# Print spearman correlation on the predicted output of the dev and test sets.\n",
        "X_train2 = np.column_stack((feature_1_train2,feature_2_train2))\n",
        "Y_train2 = np.array(df_train['score'])\n",
        "\n",
        "X_dev2 = np.column_stack((feature_1_dev2,feature_2_dev2))\n",
        "Y_dev2 = df_dev.score\n",
        "\n",
        "X_test2 = np.column_stack((feature_1_test2,feature_2_test2))\n",
        "Y_test2 = df_test.score\n",
        "\n",
        "reg2 = RandomForestRegressor()\n",
        "reg2.fit(X_train2, Y_train2)\n",
        "print(reg2.score(X_train2, Y_train2))\n",
        "\n",
        "print(spearmanr(reg2.predict(X_dev2), Y_dev2))\n",
        "print(spearmanr(reg2.predict(X_test2), Y_test2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VImljTWps_GR",
      "metadata": {
        "id": "VImljTWps_GR"
      },
      "source": [
        "## CONFIGURATION 3: Fine-Tune a Contextual Embeddings Model: 18 marks\n",
        "1 Prepare data samples to be for the DL model to consume. Add the code in the `form_data()`. **4 marks**\n",
        "\n",
        "3 Create the data loader, one each for train/dev/test data_input sample set obtained from `form_input_example()`. **1.5 marks**\n",
        "\n",
        "4 Initiate `model3` consisting of **atleast** the following 3 components - `base_LM`, a `pooling_layer` and a `dense_layer`. Use appropriate activation function in dense. **Atleast** one layer of `base_LM` should be set to trainable. **5 marks**\n",
        "\n",
        "6 Initiate the `loss`. **0.5 marks**\n",
        "\n",
        "7 Fit the `model3`. Use `NUM_EPOCHS = 2`. **MAX_NUM_EPOCHS allowed will be 3**. **2 marks** \n",
        "\n",
        "8 Complete the `get_model_predicts()` to obtain predicted scores for input sentence pairs. **3.5 marks** \n",
        "\n",
        "9 Print the correlation scores on the dev and test set predictions. **1.5 mark**\n",
        "\n",
        "Useful References: https://huggingface.co/blog/how-to-train-sentence-transformers "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0kb0xJZmZGIR",
      "metadata": {
        "id": "0kb0xJZmZGIR"
      },
      "outputs": [],
      "source": [
        "def form_data(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the dataloder.\n",
        "  \"\"\"\n",
        "\n",
        "def get_model_predicts(data_type, trained_model):\n",
        "  \"\"\"\n",
        "  Input the dataset list and return a list of cosine similarity scores. Use the fitted final_trainable_model for obtaining encodings.\n",
        "  \"\"\"\n",
        "\n",
        "# dataloader_<dataset_type> = form_data(data_frame)\n",
        "# base_model = \n",
        "# layer_ppoling = \n",
        "# layer_dense = \n",
        "# model3 = \n",
        "# loss =\n",
        "\n",
        "# Fit the model3.\n",
        "# Print spearman correlation on the predicted output of the dev and test sets."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "d67b38e366ca74dac447d11a031a36b4e680cd6b313aec980c02ee663b193322"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
