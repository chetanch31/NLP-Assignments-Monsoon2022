{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "with open(\"A1_dataset.csv\", encoding=\"utf8\", newline='') as file:\n",
    "    data = csv.reader(file)\n",
    "    next(data)\n",
    "    data_list = list(data)\n",
    "    \n",
    "negative_sent = []\n",
    "positive_sent = []\n",
    "\n",
    "for line in data_list:\n",
    "    if line[0][0] == \"0\":\n",
    "        negative_sent.append(line)\n",
    "\n",
    "    else:\n",
    "        positive_sent.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt = \"heya  j jdbid sjk c  c\"\n",
    "reg_exp = \"\\s\\s+\"\n",
    "negative_pre = []\n",
    "positive_pre = []\n",
    "\n",
    "for i in range(len(negative_sent)):\n",
    "    x = re.sub(reg_exp, \" \", negative_sent[i][2] )\n",
    "    #TODO remove this\n",
    "    x = x.lower()\n",
    "    negative_pre.append([negative_sent[i][0],negative_sent[i][1], x])\n",
    "    # print(negative_pre[i][2])\n",
    "    \n",
    "\n",
    "# print(negative_pre)\n",
    "# x = re.sub(\"\\s\\s+\", \" \", txt )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp = \"((https?|www[\\.][^\\.\\s])[^\\s]+)\"\n",
    "# txt = \"Output http://bit.ly/AEbs3 \"\n",
    "\n",
    "# x = re.sub(reg_exp, \"AAAAAAA\", txt)\n",
    "# print(x)\n",
    "\n",
    "\n",
    "for i in range(len(negative_sent)): #Remove URLs\n",
    "    x = re.sub(reg_exp, \"\", negative_pre[i][2])\n",
    "    negative_pre[i][2] = x\n",
    "    # print(negative_pre[i][2])\n",
    "\n",
    "# # tokens = re.findall(reg_exp, )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp = \"[@]\\w+\"\n",
    "# txt = \"Output http://bit.ly/AEbs3 \"\n",
    "\n",
    "# x = re.sub(reg_exp, \"AAAAAAA\", txt)\n",
    "# print(x)\n",
    "\n",
    "\n",
    "for i in range(len(negative_sent)): #Remove Usernames\n",
    "    x = re.sub(reg_exp, \"\", negative_pre[i][2])\n",
    "    negative_pre[i][2] = x\n",
    "    # print(negative_pre[i][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuations and Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp = \"&quot|&gt|&amp|&lt\"\n",
    "\n",
    "for i in range(len(negative_sent)): \n",
    "    x = re.sub(reg_exp, \"\", negative_pre[i][2])\n",
    "    negative_pre[i][2] = x\n",
    "    # print(negative_pre[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp = \"[^a-zA-Z0-9\\s]\"\n",
    "\n",
    "for i in range(len(negative_sent)): \n",
    "    x = re.sub(reg_exp, \" \", negative_pre[i][2])\n",
    "    negative_pre[i][2] = x\n",
    "    # print(negative_pre[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp = r\"\\b\\w*\\d\\w*\\b\"\n",
    "\n",
    "for i in range(len(negative_sent)): \n",
    "    x = re.sub(reg_exp, \" \", negative_pre[i][2])\n",
    "    negative_pre[i][2] = x\n",
    "    # print(negative_pre[i][2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = list(stopwords.words('english'))\n",
    "extra_stop_words = [\"u\",\"ur\",\"uk\",\"yeah\",\"yea\",\"x\",\"xxx\",\"l\",\"r\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"j\",\"k\",\"m\",\"n\",\"o\",\"p\",\"q\",\"v\",\"w\",\"y\",\"z\"]\n",
    "\n",
    "stop_words = stop_words + extra_stop_words\n",
    "# print(stop_words)\n",
    "# stop_words_string = \"|\".join(stop_words)\n",
    "# print(stop_words_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_exp = new regExp'\\b('+ stop_words_string + ')\\b'\n",
    "\n",
    "\n",
    "reg_exp = r\"\\b(\" + r\"|\".join(stop_words) + r\")\\b\"\n",
    "# re.findall(prep_re, paragraph, re.IGNORECASE)\n",
    "\n",
    "# print(reg_exp)\n",
    "for i in range(len(negative_sent)): \n",
    "    x = re.sub(reg_exp, \" \", negative_pre[i][2])\n",
    "    negative_pre[i][2] = x\n",
    "    # print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp = \"\\s\\s+\"\n",
    "\n",
    "# print(reg_exp)\n",
    "for i in range(len(negative_sent)): \n",
    "    x = re.sub(reg_exp, \" \", negative_pre[i][2])\n",
    "    negative_pre[i][2] = x\n",
    "    # print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_exp = \"\\s\\s+\"\n",
    "\n",
    "# print(reg_exp)\n",
    "for i in range(len(negative_sent)): \n",
    "    # x = re.sub(reg_exp, \" \", negative_pre[i][2])\n",
    "    x = negative_pre[i][2].split()\n",
    "    negative_pre[i][2] = x\n",
    "    # print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "# Downloading and importing\n",
    "# package 'words' from nltk corpus\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "  \n",
    "  \n",
    "correct_words = words.words()\n",
    "\n",
    "# list of incorrect spellings\n",
    "# that need to be corrected \n",
    "# incorrect_words=[\"wat\", 'lemme', 'intelliengt']\n",
    "  \n",
    "# loop for finding correct spellings\n",
    "# based on jaccard distance\n",
    "# and printing the correct word\n",
    "for i in range(len(negative_pre)):\n",
    "    for j in range(len(negative_pre[i][2])):\n",
    "        word = negative_pre[i][2][j]\n",
    "        temp = [(jaccard_distance(set(ngrams(word, 2)),set(ngrams(w, 2))),w)\n",
    "                for w in correct_words if w[0]==word[0]]\n",
    "        print(sorted(temp, key = lambda val:val[0])[0][1])\n",
    "        negative_pre[i][2][j] = sorted(temp, key = lambda val:val[0])[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(negative_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(negative_pre)):\n",
    "    for j in range(len(negative_pre[i][2])):\n",
    "        word = negative_pre[i][2][j]\n",
    "        word = lem.lemmatize(word, wordnet.VERB)\n",
    "        negative_pre[i][2][j] = word\n",
    "        print(word) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbab7ad93b4c816b81ea2fcadccb708733fafe3d4de9be705110c0acce2d8094"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
